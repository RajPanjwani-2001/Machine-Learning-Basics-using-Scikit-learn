{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DT_mlt.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO9nBeh7fp7o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from datetime import date\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.decomposition import *\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "class FeatureEngineering:\n",
        "\n",
        "    def data_handling(self, df):\n",
        "        enc = LabelEncoder()\n",
        "        for i in range(df.shape[1]):\n",
        "            if df.iloc[:,i].isna().sum()!=0:\n",
        "                df.iloc[:,i].fillna(df.iloc[:,i].mean(), inplace=True)\n",
        "                print(df.iloc[:,i].isna().sum())\n",
        "\n",
        "            if df.iloc[:,i].dtype == object:\n",
        "                print(\"Column contains a categorical value: \",i)\n",
        "                df.iloc[:,i] = enc.fit_transform(df.iloc[:,i])\n",
        "                print('Categories: ',df.iloc[:,i].unique())\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "    #Imputation\n",
        "\n",
        "    def impute_mean(self,df):\n",
        "        for i in range(df.shape[1]):\n",
        "            if df.iloc[:,i].isna().sum()!=0:\n",
        "                df.iloc[:,i].fillna(df.iloc[:,i].mean(), inplace=True)\n",
        "                print(df.iloc[:,i].isna().sum())\n",
        "        return df\n",
        "\n",
        "    def impute_mode(self,df):\n",
        "        for i in range(df.shape[1]):\n",
        "            if df.iloc[:,i].isna().sum()!=0:\n",
        "                df.iloc[:,i].fillna(df.iloc[:,i].mode()[0], inplace=True)\n",
        "                print(df.iloc[:,i].isna().sum())\n",
        "        return df\n",
        "\n",
        "    def impute_median(self,df,col):\n",
        "        print(\"Column contains NA value : \" + str(col))\n",
        "        df.iloc[:,col].fillna(df.iloc[:,col].median(), inplace=True)\n",
        "    \n",
        "    def impute_random(self,df,col):\n",
        "        print(\"Column contains NA value : \" + str(col))\n",
        "        random_sample = df.iloc[:,col].dropna().sample(df.iloc[:,col].isna().sum(),random_state = 0)\n",
        "        random_sample.index = df[df.iloc[:,col].isna()].index\n",
        "        df.loc[df.iloc[:,col].isna(),df.columns[col]] = random_sample\n",
        "    \n",
        "  \n",
        "    def impute_end_dist(self,df,col):\n",
        "        print(\"Column contains NA value : \" + str(col))\n",
        "        extreme = df.iloc[:,col].mean() + 3* df.iloc[:,col].std()\n",
        "        df.iloc[:,col] = df.iloc[:,col].fillna(extreme)\n",
        "\n",
        "    #LabelEncoding\n",
        "    def label_enc(self,df):\n",
        "        enc = LabelEncoder()\n",
        "        for i in range(df.shape[1]):\n",
        "            if df.iloc[:,i].dtype == object:\n",
        "                print(\"Column contains a categorical value: \",i)\n",
        "                df.iloc[:,i] = enc.fit_transform(df.iloc[:,i])\n",
        "                print('Categories: ',df.iloc[:,i].unique())\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "    #Handling Outliers\n",
        "    def outliers_std(self,df,col,factor): #same as standf[df.columns[col]] = df[(df[df.columns[col]]<ub) & (df[df.columns[col]]>lb)]dard-scaler\n",
        "        ub = df.iloc[:,col].mean() + factor * df.iloc[:,col].std()\n",
        "        lb = df.iloc[:,col].mean() - factor * df.iloc[:,col].std()\n",
        "        df[df.columns[col]] = df[(df[df.columns[col]]<ub) & (df[df.columns[col]]>lb)]\n",
        "\n",
        "    def outliers_percentile(self,df,col):\n",
        "        ub = df.iloc[:,col].quantile(0.95)\n",
        "        lb = df.iloc[:,col].quantile(0.05)\n",
        "        df[df.columns[col]] = df[(df[df.columns[col]]<ub) & (df[df.columns[col]]>lb)]\n",
        "\n",
        "    #Normalization\n",
        "    def min_max(self,df,col):\n",
        "        df_min = df.iloc[:,col].min()\n",
        "        df_max = df.iloc[:,col].max()\n",
        "\n",
        "        df.iloc[:,col] = (df.iloc[:,col] - df_min) / (df_max - df_min)\n",
        "\n",
        "    def std_scal(self,df,col):\n",
        "        df_mean = df.iloc[:,col].mean()\n",
        "        df_std = df.iloc[:,col].std()\n",
        "\n",
        "        df.iloc[:,col] = (df.iloc[:,col] - df_mean)/df_std\n",
        "\n",
        "    def avg_scal(self,df,col):\n",
        "        df_mean = df.iloc[:,col].mean()\n",
        "        df_max = df.iloc[:,col].max()\n",
        "\n",
        "        df.iloc[:,col] = (df.iloc[:,col] - df_mean) / (df_max - df_mean)\n",
        "    \n",
        "    #Discritization\n",
        "    def binning_uniform(self,data,n_bins):\n",
        "        obj = KBinsDiscretizer(n_bins=n_bins , encode='ordinal',strategy='uniform')\n",
        "        data = obj.fit_transform(data)\n",
        "        return data\n",
        "\n",
        "    def binning_quantile(self,data,n_bins):\n",
        "        obj = KBinsDiscretizer(n_bins=n_bins , encode='ordinal',strategy='quantile')\n",
        "        data = obj.fit_transform(data)\n",
        "        return data\n",
        "\n",
        "    def binning_kmeans(self,data,n_bins):\n",
        "        obj = KBinsDiscretizer(n_bins=n_bins , encode='ordinal',strategy='kmeans')\n",
        "        data = obj.fit_transform(data)\n",
        "        return data\n",
        "\n",
        "    #Feature Selection    \n",
        "\n",
        "    def factor_analysis(self,data,n_components):\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(data)\n",
        "\n",
        "        scaled_data = scaler.transform(data)\n",
        "        fa_obj = FactorAnalysis(n_components=n_components,random_state=0)\n",
        "        fa_obj.fit(scaled_data)\n",
        "        x = fa_obj.transform(scaled_data)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def pca(self,data,n_components):\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(data)\n",
        "\n",
        "        scaled_data = scaler.transform(data)\n",
        "        pca_obj = PCA(n_components=n_components)\n",
        "        pca_obj.fit(scaled_data)\n",
        "        x_pca = pca_obj.transform(scaled_data)\n",
        "        return x_pca\n",
        "\n",
        "    def fast_ica(self,data,n_components):\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(data)\n",
        "\n",
        "        scaled_data = scaler.transform(data)\n",
        "        fi_obj = FastICA(n_components=n_components,random_state=0)\n",
        "        fi_obj.fit(scaled_data)\n",
        "        x = fi_obj.transform(scaled_data)\n",
        "        return x\n",
        "   \n",
        "    def incremental_pca(self,data,n_components):\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(data)\n",
        "\n",
        "        scaled_data = scaler.transform(data)\n",
        "        obj = IncrementalPCA(n_components=n_components)\n",
        "        obj.fit(scaled_data)\n",
        "        x = obj.transform(scaled_data)\n",
        "        return x\n",
        "        \n",
        "\n",
        "    def kernel_pca(self,data,n_components,kernel='linear'): #kernel = ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘cosine’, ‘precomputed’\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(data)\n",
        "\n",
        "        scaled_data = scaler.transform(data)\n",
        "        obj = KernelPCA(n_components=n_components,kernel=kernel)\n",
        "        obj.fit(scaled_data)\n",
        "        x = obj.transform(scaled_data)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def lda(self,data,n_components):\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(data)\n",
        "\n",
        "        scaled_data = scaler.transform(data)\n",
        "        obj = LatentDirichletAllocation(n_components=n_components,random_state=0)\n",
        "        obj.fit(scaled_data)\n",
        "        x = obj.transform(scaled_data)\n",
        "        return x\n",
        "\n",
        "    def mini_batch_dict_learning(self,data,n_components,transform_algorithm= 'lasso_lars'): #transform_algorithm = 'lasso_lars','lasso_cd'\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(data)\n",
        "\n",
        "        scaled_data = scaler.transform(data)\n",
        "        obj = MiniBatchDictionaryLearning(n_components=n_components,transform_algorithm=transform_algorithm)\n",
        "        obj.fit(scaled_data)\n",
        "        x = obj.transform(scaled_data)\n",
        "        return x\n",
        "        \n",
        "    def mini_batch_sparse_pca(self,data,n_components):\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(data)\n",
        "\n",
        "        scaled_data = scaler.transform(data)\n",
        "        obj = MiniBatchSparsePCA(n_components=n_components,random_state=0)\n",
        "        obj.fit(scaled_data)\n",
        "        x = obj.transform(scaled_data)\n",
        "        return x\n",
        "\n",
        "    def nmf(self,data,n_components,init='random'): #init{‘random’, ‘nndsvd’, ‘nndsvda’, ‘nndsvdar’, ‘custom’}\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(data)\n",
        "\n",
        "        scaled_data = scaler.transform(data)\n",
        "        obj = NMF(n_components=n_components, init=init, random_state=0)\n",
        "        obj.fit(scaled_data)\n",
        "        x = obj.transform(scaled_data)\n",
        "        return x\n",
        "\n",
        "    def sparse_pca(self,data,n_components):\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(data)\n",
        "\n",
        "        scaled_data = scaler.transform(data)\n",
        "        obj = SparsePCA(n_components=n_components,random_state=0)\n",
        "        obj.fit(data)\n",
        "        x = obj.transform(data)\n",
        "        return x\n",
        "\n",
        "    def truncated_svd(self,data,n_components):\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(data)\n",
        "\n",
        "        scaled_data = scaler.transform(data)\n",
        "        obj = TruncatedSVD(n_components=n_components,random_state=0)\n",
        "        obj.fit(scaled_data)\n",
        "        x = obj.transform(scaled_data)\n",
        "        return x\n",
        "\n",
        "    #metrics\n",
        "    def calc_acc(self,model,X_test,Y_test):\n",
        "        y_pred = model.predict(X_test) \n",
        "        return accuracy_score(Y_test,y_pred),y_pred\n",
        "\n",
        "\n",
        "    '''def date_col(self,df,col):df\n",
        "        #Transform string to date\n",
        "        df['date'] = pd.to_datetime(df.date, format=\"%d-%m-%Y\")\n",
        "\n",
        "        #Extracting Year\n",
        "        df['year'] = df['date'].dt.year\n",
        "\n",
        "        #Extracting Month\n",
        "        df['month'] = df['date'].dt.month\n",
        "\n",
        "        #Extracting passed years since the date\n",
        "        df['passed_years'] = date.today().year - df['date'].dt.year\n",
        "\n",
        "        #Extracting passed months since the date\n",
        "        df['passed_months'] = (date.today().year - df['date'].dt.year) * 12 + date.today().month - df['date'].dt.month\n",
        "\n",
        "        #Extracting the weekday name of the date\n",
        "        df['day_name'] = df['date'].dt.day_name()'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv('salaries.csv')\n",
        "obj = FeatureEngineering()\n",
        "\n",
        "df = obj.data_handling(df)\n",
        "data = df.values\n",
        "\n",
        "x = data[:,:-1]\n",
        "y = data[:,-1]\n",
        "\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train,Y_train)\n",
        "\n",
        "ac,y_pred = obj.calc_acc(model,X_test,Y_test)\n",
        "print('Accuracy : ',ac)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUrFVbw2iVfo",
        "outputId": "fa895a86-35f6-4daf-9f60-dcf5ec61d1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column contains a categorical value:  0\n",
            "Categories:  [2 0 1]\n",
            "Column contains a categorical value:  1\n",
            "Categories:  [2 0 1]\n",
            "Column contains a categorical value:  2\n",
            "Categories:  [0 1]\n",
            "Accuracy :  0.5\n"
          ]
        }
      ]
    }
  ]
}